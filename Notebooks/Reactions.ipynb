{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907aceed",
   "metadata": {},
   "source": [
    "# Reactions\n",
    "This notebook contains all the steps followed in order to reconcile the existing metabolic reconstruction in CHO cells. It si divided into two parts: **1. Network Reconstruction** and **2. Identification of duplicated reactions**\n",
    "\n",
    "[1. Network Reconstruction](#reconstruction) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.1 Datasets generation and merge**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.2 Normalization of the data** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.3 Group all the data into a unified dataset** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.4 Addition of the Recon3D CHO ortholog GPRs into the reconstruction** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.5 Divide the dataset in two to facilitate reading and curation in Google Sheets** <br>\n",
    "\n",
    "[2. Identification of Duplicated Reactions](#duplicated) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.1 Generate the model and identify duplicated reactions**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.2 Fix duplicated reactions in the dataset from the list duplicated_reactions obtained above** <br>\n",
    "\n",
    "[3. Add BiGG database links to every reaction](#bigg) <br>\n",
    "\n",
    "[4. Divide into compartments](#compartments) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.1 Generate the model and identify duplicated reactions**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.2 Fix duplicated reactions in the dataset from the list duplicated_reactions obtained above** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0896294",
   "metadata": {},
   "source": [
    "## 1. Network Reconstruction <a id='reconstruction'></a>\n",
    "These include the generation of datasets (1), normalization of the data (2), merging all four reconstructions into a unified dataset (3), compiling the generated reconstruction into a cobra model in order to identify duplicated reactions (4), fixing duplicated reactions using BiGG ID annotation (5), adding Recon3D GPR information (6), and finally dividing the dataset into two different datasets that will be further curated in Google Sheets (7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce2b55",
   "metadata": {},
   "source": [
    "### 1.1 Datasets generation and merge\n",
    "Dataset generation from previous reconstructions (CHO_DG44, iCHO1766, iCHO2101, iCHO2291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read excel files and create the dfs\n",
    "\n",
    "#Camels CHO_DG44 metabolic reconstruction\n",
    "camel_df = pd.read_excel('../Data/Reconciliation/datasets/CHO_DG44.xlsx', header = 1)\n",
    "\n",
    "#Hefzi's iCHO1766 metabolic reconstruction\n",
    "hefzi_df = pd.read_excel('../Data/Reconciliation/datasets/hefzi_final.xlsx')\n",
    "\n",
    "#Foudaliha's iCHO2101 metabolic reconstruction\n",
    "fouladiha_df = pd.read_excel('../Data/Reconciliation/datasets/iCHO2101.xlsx', 'Supplementary Table 10', header = 1)\n",
    "\n",
    "#Yeo's iCHO2291 metabolic reconstruction\n",
    "iCHO2291 = pd.read_excel('../Data/Reconciliation/datasets/iCHO2291_final.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eca726",
   "metadata": {},
   "source": [
    "### 1.2 Normalization of the data\n",
    "All dataset are normalized into the same shape and format and then combined into one big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarization of the columns names\n",
    "camel_df.rename(columns = {'Reaction ID':'Reaction', 'Initial reaction in model':'Reaction Formula', 'Reaction name':'Reaction Name', 'Justification':'Curation Notes'}, inplace = True)\n",
    "fouladiha_df.rename(columns = {'Abbreviation':'Reaction', 'Description':'Reaction Name', 'Reaction':'Reaction Formula', 'GPR':'GPR_fou'}, inplace = True)\n",
    "\n",
    "# Addition of tag columns for organizational purposes\n",
    "camel_df.insert(loc=0, column='cam', value='X')\n",
    "camel_df.insert(loc=1, column='hef', value=np.nan)\n",
    "camel_df.insert(loc=2, column='fou', value=np.nan)\n",
    "camel_df.insert(loc=3, column='yeo', value=np.nan)\n",
    "\n",
    "hefzi_df.insert(loc=0, column='cam', value=np.nan)\n",
    "hefzi_df.insert(loc=1, column='hef', value='X')\n",
    "hefzi_df.insert(loc=2, column='fou', value=np.nan)\n",
    "hefzi_df.insert(loc=3, column='yeo', value=np.nan)\n",
    "\n",
    "fouladiha_df.insert(loc=0, column='cam', value=np.nan)\n",
    "fouladiha_df.insert(loc=1, column='hef', value=np.nan)\n",
    "fouladiha_df.insert(loc=2, column='fou', value='X')\n",
    "fouladiha_df.insert(loc=3, column='yeo', value=np.nan)\n",
    "\n",
    "iCHO2291.insert(loc=0, column='cam', value=np.nan)\n",
    "iCHO2291.insert(loc=1, column='hef', value=np.nan)\n",
    "iCHO2291.insert(loc=2, column='fou', value=np.nan)\n",
    "iCHO2291.insert(loc=3, column='yeo', value='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cols list with the column names from all datasets\n",
    "cols = hefzi_df.columns.to_list()+fouladiha_df.columns.to_list()+iCHO2291.columns.to_list()+camel_df.columns.to_list()\n",
    "\n",
    "# Eliminate repetitive values in the 'cols' list\n",
    "cols = [cols[i] for i in range(len(cols)) if i == cols.index(cols[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col(df):\n",
    "    '''\n",
    "    This function adds the columns from the cols list \n",
    "    that are not present in the df\n",
    "    '''\n",
    "    df.columns\n",
    "    add_col = []\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            add_col.append(col)\n",
    "    df = df.reindex(columns = df.columns.tolist() + add_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify columns for all datasets\n",
    "hefzi_df = add_col(hefzi_df)\n",
    "fouladiha_df = add_col(fouladiha_df)\n",
    "iCHO2291 = add_col(iCHO2291)\n",
    "camel_df = add_col(camel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns in all datasets the same way\n",
    "fouladiha_df = fouladiha_df[['cam','hef', 'fou', 'yeo', 'Reaction', 'Reaction Name', 'Reaction Formula', 'GPR_hef','GPR_fou','GPR_yeo', 'Subsystem', 'Genes', 'Protein', 'EC Number', 'Mol wt', 'kcat_forward', 'kcat_backward', 'Reversible','Lower bound', 'Upper bound', 'Objective', 'Curation Notes', 'References', 'Reaction ID Camels Models']]\n",
    "iCHO2291 = iCHO2291[['cam','hef', 'fou', 'yeo', 'Reaction', 'Reaction Name', 'Reaction Formula', 'GPR_hef','GPR_fou','GPR_yeo', 'Subsystem', 'Genes', 'Protein', 'EC Number', 'Mol wt', 'kcat_forward', 'kcat_backward', 'Reversible','Lower bound', 'Upper bound', 'Objective', 'Curation Notes', 'References', 'Reaction ID Camels Models']]\n",
    "hefzi_df = hefzi_df[['cam','hef', 'fou', 'yeo', 'Reaction', 'Reaction Name', 'Reaction Formula', 'GPR_hef','GPR_fou','GPR_yeo', 'Subsystem', 'Genes', 'Protein', 'EC Number', 'Mol wt', 'kcat_forward', 'kcat_backward', 'Reversible','Lower bound', 'Upper bound', 'Objective', 'Curation Notes', 'References', 'Reaction ID Camels Models']]\n",
    "camel_df = camel_df[['cam','hef', 'fou', 'yeo', 'Reaction', 'Reaction Name', 'Reaction Formula', 'GPR_hef','GPR_fou','GPR_yeo', 'Subsystem', 'Genes', 'Protein', 'EC Number', 'Mol wt', 'kcat_forward', 'kcat_backward', 'Reversible','Lower bound', 'Upper bound', 'Objective', 'Curation Notes', 'References', 'Reaction ID Camels Models']]\n",
    "\n",
    "camel_df['Reaction'] = camel_df['Reaction'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the dfs into a unified df\n",
    "all_dfs = pd.concat([camel_df, hefzi_df, fouladiha_df, iCHO2291])\n",
    "all_dfs = all_dfs.reset_index(drop = True)\n",
    "\n",
    "#Unify reaction names\n",
    "all_dfs['Reaction'] = all_dfs['Reaction'].str.replace('_cho', '')\n",
    "all_dfs['Reaction'] = all_dfs['Reaction'].str.replace(r\"(e)\", \"_e_\", regex = False)\n",
    "all_dfs['Reaction'] = all_dfs['Reaction'].str.replace(\"[\", \"_\")\n",
    "all_dfs['Reaction'] = all_dfs['Reaction'].str.replace(\"]\", \"_\")\n",
    "\n",
    "all_dfs #20940 rows/reactions (many of them repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044513b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the underscore at the end of some reactions\n",
    "\n",
    "rxns = []\n",
    "idx = []\n",
    "\n",
    "for i,row in all_dfs.iterrows():\n",
    "    if str(row['Reaction']).endswith('_'):\n",
    "        s = re.sub('_$', '', row['Reaction'])\n",
    "        rxns.append(s)\n",
    "        idx.append(i)\n",
    "        \n",
    "all_dfs['Reaction'].update(pd.Series(rxns,index=idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119e796",
   "metadata": {},
   "source": [
    "### 1.3 Group all the data into a unified dataset\n",
    "The combined dataset generated above \"all_dfs\" is grouped by the Reaction BiGG ID to obtain a dataset with unique reaction identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data into a unified dataset\n",
    "\n",
    "all_dfs2 = all_dfs.groupby('Reaction').first()\n",
    "all_dfs2['Reaction Formula'] = all_dfs2['Reaction Formula'].str.replace('[','_')\n",
    "all_dfs2['Reaction Formula'] = all_dfs2['Reaction Formula'].str.replace(']','')\n",
    "all_dfs2['Reaction Formula'] = all_dfs2['Reaction Formula'].str.replace(' => | =>',' --> ')\n",
    "all_dfs2['Reaction Formula'] = all_dfs2['Reaction Formula'].str.replace(' <-- | <--',' <=> ')\n",
    "all_dfs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Demand Reaccion into a different df and remove from reconstruction. \n",
    "# Keep demand reactions from extracellular space\n",
    "\n",
    "all_dfs2 = all_dfs2.reset_index()\n",
    "demand_reactions = pd.DataFrame(columns = all_dfs2.columns)\n",
    "\n",
    "for index, rxn in all_dfs2.iterrows():\n",
    "    rxn = rxn['Reaction Formula']\n",
    "    a,b = re.split('<=>|-->',rxn)\n",
    "    if (b == '' or b == ' ') and not a.endswith('_e '):\n",
    "        temp_df = all_dfs2[all_dfs2['Reaction Formula'] == rxn]\n",
    "        demand_reactions = pd.concat([demand_reactions,temp_df])\n",
    "        all_dfs2.drop(index, inplace=True)\n",
    "\n",
    "demand_reactions = demand_reactions.reset_index(drop=True)\n",
    "demand_reactions.to_excel('../Data/Reconciliation/datasets/demand_reactions.xlsx')\n",
    "demand_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs2 = all_dfs2.reset_index(drop=True)\n",
    "all_dfs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7ae59",
   "metadata": {},
   "source": [
    "### 1.4 Addition of the Recon3D CHO ortholog GPRs into the reconstruction\n",
    "The information from the dataset generated in the notebook \"GPR Annotation\" containing all the information from Recon3D GPRS in human and its corresponding CHO orthologs is mapped into our reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19817d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dict with recon3d reactions as keys and the CHO GPRs as values.\n",
    "recon3d = pd.read_excel('../Data/GPR_Curation/recon3D_chogprs.xlsx')\n",
    "recon3d_dict = recon3d.set_index('m_reaction')['CHO GPR'].to_dict()\n",
    "recon3d_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'recon3d_dict' into the all_dfs3 dataframe\n",
    "# the reaction IDs should be the same as those in our reconstruction\n",
    "\n",
    "all_dfs3 = all_dfs2.reset_index()\n",
    "all_dfs3['GPR_Recon3D'] = all_dfs3['Reaction'].map(recon3d_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a607",
   "metadata": {},
   "source": [
    "### 1.5 Divide the dataset in two to facilitate reading and curation in Google Sheets\n",
    "The dataset all_dfs3 is divided into two different datasets: all_dfs4 contains mainly all the information regarding GPRs assigned from previous reconstructions. all_dfs5 contains the rest of the attributes in the reconstruction such as EC number, bounds, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90db5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs4 contains mainly information of the reactions GPRs\n",
    "all_dfs4 = all_dfs3[['Reaction', 'Reaction Name', 'Reaction Formula', 'Subsystem', 'GPR_hef', 'GPR_fou', 'GPR_yeo', 'GPR_Recon3D', 'Curation Notes', 'References']]\n",
    "all_dfs4.insert(8,'GPR_final', '')\n",
    "all_dfs4.to_excel('../Data/Reconciliation/datasets/all_dfs4.xlsx')\n",
    "all_dfs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65952ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs5 contains information of the rest of the attributes in our reconstruction\n",
    "all_dfs5 = all_dfs3[['Reaction', 'Genes', 'Protein', 'EC Number', 'Mol wt', 'kcat_forward', 'kcat_backward', 'Reversible', 'Lower bound', 'Upper bound', 'Objective']]\n",
    "all_dfs5.to_excel('../Data/Reconciliation/datasets/all_dfs5.xlsx')\n",
    "all_dfs5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb752ef7",
   "metadata": {},
   "source": [
    "## 2. Identifications of Duplicated Reactions <a id='duplicated'></a>\n",
    "In this part of the notebook we use the **duplicated_reactions** fucntion from the **utils** module to spot duplicated reactions in our dataset. First we generate a model, using cobprapy, with our dataset, then we apply the **duplicated_reactions** function to the model, and finally we standarize the name of the duplicated reactions according to the nomenclature used in BiGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36be791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import HTMLSession\n",
    "from tqdm.notebook import tqdm\n",
    "from cobra import Model, Reaction, Metabolite\n",
    "\n",
    "from utils import duplicated_reactions\n",
    "from google_sheet import GoogleSheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd8ed0",
   "metadata": {},
   "source": [
    "### 2.1 Generate the model and identify duplicated reactions\n",
    "Generation of a cobra model from the **Google Sheet dataset**. This model will be used to identify duplicated reactions from the stoichiometric matrix of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9c0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the necessary datasets for the identification of the duplicated reactions\n",
    "KEY_FILE_PATH = 'credentials.json'\n",
    "SPREADSHEET_ID_v3 = '1MlBXeHIKw8k8fZyXm-sN__AHTRSunJxar_-bqvukZws'\n",
    "\n",
    "# Initialize the GoogleSheet object\n",
    "sheet = GoogleSheet(SPREADSHEET_ID_v3, KEY_FILE_PATH)\n",
    "\n",
    "# Read data from the Google Sheet\n",
    "sheet_rxns = 'Rxns'\n",
    "sheet_attributes = 'Attributes'\n",
    "\n",
    "reactions = sheet.read_google_sheet(sheet_rxns)\n",
    "rxns_attributes = sheet.read_google_sheet(sheet_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Create a model from the reactions sheet ----- #####\n",
    "model = Model(\"iCHO\")\n",
    "lr = []\n",
    "for _, row in reactions.iterrows():\n",
    "    r = Reaction(row['Reaction'])\n",
    "    lr.append(r)    \n",
    "model.add_reactions(lr)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00893fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Add information to each one of the reactions ----- #####\n",
    "for i,r in enumerate(tqdm(model.reactions)):\n",
    "    print(r.id)\n",
    "    r.build_reaction_from_string(reactions['Reaction Formula'][i])\n",
    "    r.name = reactions['Reaction Name'][i]\n",
    "    r.subsystem = reactions['Subsystem'][i]\n",
    "    r.lower_bound = float(rxns_attributes['Lower bound'][i])\n",
    "    r.upper_bound = float(rxns_attributes['Upper bound'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ff757",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_reactions = duplicated_reactions(model)\n",
    "duplicated_reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d684b4",
   "metadata": {},
   "source": [
    "### 2.2 Fix duplicated reactions in the dataset from the list duplicated_reactions obtained above\n",
    "Duplicated reactions are itereated in a for loop and mapped in the original dataset. A request is made in the \n",
    "BiGG database http://bigg.ucsd.edu with each of the duplicated reactions. If any of the duplicated reactions is in BiGG, the other reaction automatically changes its name to the one located in Bigg. This way we unifiy the names of our reactions to those in BiGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for rxn in tqdm(duplicated_reactions):\n",
    "    \n",
    "    session = HTMLSession()\n",
    "    \n",
    "    response=session.get('http://bigg.ucsd.edu/models/iCHOv1/reactions/'+reactions['Reaction'][rxn[0]])\n",
    "    if response.status_code == 200:\n",
    "        if reactions['Reaction'][rxn[0]] == rxns_attributes['Reaction'][rxn[0]] and reactions.loc[rxn[1], 'Reaction'] == rxns_attributes.loc[rxn[1], 'Reaction']:\n",
    "            print(f'1 Reaction {reactions.iloc[rxn[1],2]} changed for {reactions.iloc[rxn[0],2]} present in CHOv1 model')\n",
    "            reactions.loc[rxn[1], 'Reaction'] = reactions['Reaction'][rxn[0]]\n",
    "            rxns_attributes.loc[rxn[1], 'Reaction'] = rxns_attributes['Reaction'][rxn[0]]\n",
    "            i += 1\n",
    "    \n",
    "    elif response.status_code != 200:\n",
    "        response=session.get('http://bigg.ucsd.edu/models/iCHOv1/reactions/'+reactions['Reaction'][rxn[1]])\n",
    "        if response.status_code == 200:\n",
    "            if reactions['Reaction'][rxn[0]] == rxns_attributes['Reaction'][rxn[0]] and reactions.loc[rxn[1], 'Reaction'] == rxns_attributes.loc[rxn[1], 'Reaction']:\n",
    "                print(f'2 Reaction {reactions.iloc[rxn[0],2]} changed for {reactions.iloc[rxn[1],2]} present in CHOv1 model')\n",
    "                reactions.loc[rxn[0], 'Reaction'] = reactions['Reaction'][rxn[1]]\n",
    "                rxns_attributes.loc[rxn[0], 'Reaction'] = rxns_attributes['Reaction'][rxn[1]]\n",
    "                i += 1\n",
    "            \n",
    "        elif response.status_code != 200:\n",
    "            response=session.get('http://bigg.ucsd.edu/universal/reactions/'+reactions['Reaction'][rxn[0]])\n",
    "            if response.status_code == 200:\n",
    "                if reactions['Reaction'][rxn[0]] == rxns_attributes['Reaction'][rxn[0]] and reactions.loc[rxn[1], 'Reaction'] == rxns_attributes.loc[rxn[1], 'Reaction']:\n",
    "                    print(f'3 Reaction {reactions.iloc[rxn[1],2]} changed for {reactions.iloc[rxn[0],2]} present BiGG database')\n",
    "                    reactions.loc[rxn[1], 'Reaction'] = reactions['Reaction'][rxn[0]]\n",
    "                    rxns_attributes.loc[rxn[1], 'Reaction'] = rxns_attributes['Reaction'][rxn[0]]\n",
    "                    i += 1\n",
    "                \n",
    "            elif response.status_code != 200:\n",
    "                response=session.get('http://bigg.ucsd.edu/universal/reactions/'+reactions['Reaction'][rxn[1]])\n",
    "                if response.status_code == 200:\n",
    "                    if reactions['Reaction'][rxn[0]] == rxns_attributes['Reaction'][rxn[0]] and reactions.loc[rxn[1], 'Reaction'] == rxns_attributes.loc[rxn[1], 'Reaction']:\n",
    "                        print(f'4 Reaction {reactions.iloc[rxn[0],2]} changed for {reactions.iloc[rxn[1],2]} present BiGG database')\n",
    "                        reactions.loc[rxn[0], 'Reaction'] = reactions['Reaction'][rxn[1]]\n",
    "                        rxns_attributes.loc[rxn[0], 'Reaction'] = rxns_attributes['Reaction'][rxn[1]]\n",
    "                        i += 1\n",
    "                elif response.status_code != 200:\n",
    "                    if reactions['Reaction'][rxn[0]] == rxns_attributes['Reaction'][rxn[0]] and reactions.loc[rxn[1], 'Reaction'] == rxns_attributes.loc[rxn[1], 'Reaction']:\n",
    "                        print(f'5 Reaction {reactions.iloc[rxn[1],2]} changed for {reactions.iloc[rxn[0],2]} not present in Bigg DB')\n",
    "                        reactions.loc[rxn[1], 'Reaction'] = reactions['Reaction'][rxn[0]]\n",
    "                        rxns_attributes.loc[rxn[1], 'Reaction'] = rxns_attributes['Reaction'][rxn[0]]\n",
    "                        i += 1\n",
    "\n",
    "print(len(duplicated_reactions))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f770d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the original column order\n",
    "column_order_rxns = reactions.columns.tolist()\n",
    "column_order_att = rxns_attributes.columns.tolist()\n",
    "\n",
    "# Group by 'BiGG ID' and keep the first non-null value in each group, then reset the index\n",
    "reactions = reactions.groupby('Reaction').first().reset_index()\n",
    "rxns_attributes = rxns_attributes.groupby('Reaction').first().reset_index()\n",
    "\n",
    "# Rearrange the columns to the original order\n",
    "reactions = reactions[column_order_rxns]\n",
    "rxns_attributes = rxns_attributes[column_order_att]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b641c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#### ---------------------------------------------------- ####\n",
    "#### ---- Update Rxns and  Attributes Google Sheets ----- ####\n",
    "#### ---------------------------------------------------- ####\n",
    "##############################################################\n",
    "sheet.update_google_sheet(sheet_rxns, reactions)\n",
    "sheet.update_google_sheet(sheet_attributes, rxns_attributes)\n",
    "print(\"Google Sheet updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a101d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Reactions IDs and formulas are equal in \"Rxns\" and \"Attributes\" sheets\n",
    "reactions = sheet.read_google_sheet(sheet_rxns)\n",
    "rxns_attributes = sheet.read_google_sheet(sheet_attributes)\n",
    "\n",
    "rxnsIDseq = list(reactions['Reaction']) == list(rxns_attributes['Reaction'])\n",
    "if rxnsIDseq:\n",
    "    print('Reaction IDs in the Rxns and Attributes Sheets are equal\\n')\n",
    "else:\n",
    "    rxns_sheet_ids = set(list(reactions['Reaction']))\n",
    "    attr_sheet_ids = set(list(rxns_attributes['Reaction']))\n",
    "    print(f'Reaction IDs that are in Rxns Sheet and not in Attributes Sheet {rxns_sheet_ids - attr_sheet_ids}\\n')\n",
    "    print(f'Reaction IDs that are in Attributes Sheet and not in Rxns Sheet {attr_sheet_ids - rxns_sheet_ids}\\n')\n",
    "\n",
    "rxnsforseq = list(reactions['Reaction Formula']) == list(rxns_attributes['Reaction Formula'])\n",
    "if rxnsforseq:\n",
    "    print('Reaction Formulas in the Rxns and Attributes Sheets are equal')\n",
    "else:\n",
    "    rxns_sheet_forms = set(list(reactions['Reaction Formula']))\n",
    "    attr_sheet_forms = set(list(rxns_attributes['Reaction Formula']))\n",
    "    print(f'Reaction formulas that are in Rxns Sheet and not in Attributes Sheet {rxns_sheet_forms - attr_sheet_forms}\\n')\n",
    "    print(f'Reaction formulas that are in Attributes Sheet and not in Rxns Sheet {attr_sheet_forms - rxns_sheet_forms}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f5a3e",
   "metadata": {},
   "source": [
    "## 3. Add BiGG database links to every reaction <a id='bigg'></a>\n",
    "Here we identify which reactions from our dataset are located in BiGG and we add a direct link to that reaction in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67192b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from utils import duplicated_reactions\n",
    "from google_sheet import GoogleSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url, max_retries=5):\n",
    "    session = HTMLSession()\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = session.get(url)\n",
    "            if response.status_code == 200:\n",
    "                ec_numbers = extract_ec_numbers(response)\n",
    "                return response,ec_numbers\n",
    "            elif response.status_code != 200:\n",
    "                response = None\n",
    "                ec_numbers = None\n",
    "                return response,ec_numbers\n",
    "            else:\n",
    "                retries += 1\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error occurred: {e}, retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(2)  # wait for 2 seconds before next retry\n",
    "            response = None\n",
    "            ec_numbers = None\n",
    "    return response,ec_numbers\n",
    "\n",
    "def extract_ec_numbers(response):\n",
    "    # Initialize BeautifulSoup with the response content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all elements containing EC Numbers\n",
    "    ec_number_elements = soup.find_all('a', href=lambda x: x and 'ec-code' in x)\n",
    "    \n",
    "    if len(ec_number_elements) == 0:\n",
    "        print('No EC Number asociated with this rxns in BiGG')\n",
    "        return None\n",
    "    \n",
    "    elif len(ec_number_elements) > 0:\n",
    "        # Create a list to hold the EC Numbers and their links\n",
    "        ec_numbers = []\n",
    "        ec_links = []\n",
    "\n",
    "        for element in ec_number_elements:\n",
    "            # Extract the EC Number from the text of the element\n",
    "            ec_number = element.text.strip()\n",
    "            ec_numbers.append(ec_number)\n",
    "            # Extract the link from the href attribute of the element\n",
    "            link = element['href']\n",
    "            ec_links.append(link)\n",
    "        \n",
    "        ec_numbers_string = ', '.join(ec_numbers)\n",
    "        ec_links_string = ', '.join(ec_links)\n",
    "\n",
    "        return ec_numbers_string, ec_links_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the necessary datasets\n",
    "KEY_FILE_PATH = 'credentials.json'\n",
    "SPREADSHEET_ID_v3 = '1MlBXeHIKw8k8fZyXm-sN__AHTRSunJxar_-bqvukZws'\n",
    "\n",
    "# Initialize the GoogleSheet object\n",
    "sheet = GoogleSheet(SPREADSHEET_ID_v3, KEY_FILE_PATH)\n",
    "\n",
    "# Read data from the Google Sheet\n",
    "sheet_rxns = 'Rxns'\n",
    "sheet_attributes = 'Attributes'\n",
    "\n",
    "reactions = sheet.read_google_sheet(sheet_rxns)\n",
    "rxns_attributes = sheet.read_google_sheet(sheet_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_chov1=0\n",
    "in_bigg=0\n",
    "not_bigg=0\n",
    "\n",
    "for i,row in reactions.iterrows():\n",
    "    rxn=row['Reaction']\n",
    "    response_esp, ec_data_esp = fetch_url('http://bigg.ucsd.edu/models/iCHOv1/reactions/'+rxn)\n",
    "    \n",
    "    if response_esp is not None:\n",
    "        print(rxn,'in CHOv1')\n",
    "        reactions.loc[i, 'BiGG database'] = 'http://bigg.ucsd.edu/models/iCHOv1/reactions/'+rxn\n",
    "        if rxns_attributes.loc[i,'EC Number']=='':\n",
    "            if ec_data_esp:\n",
    "                rxns_attributes.loc[i,'EC Number'] = ec_data_esp[0]\n",
    "                rxns_attributes.loc[i,'EC Number Link'] = ec_data_esp[1]\n",
    "                print(f'{rxn} EC Number found: {ec_data_esp[0]}, {ec_data_esp[1]}')\n",
    "        in_chov1+=1\n",
    "        \n",
    "    else:\n",
    "        response_gen, ec_data_gen = fetch_url('http://bigg.ucsd.edu/universal/reactions/'+rxn)\n",
    "        if response_gen is not None:\n",
    "            print(rxn,'in Bigg')\n",
    "            reactions.loc[i, 'BiGG database'] = 'http://bigg.ucsd.edu/universal/reactions/'+rxn\n",
    "            if rxns_attributes.loc[i,'EC Number']=='':\n",
    "                if ec_data_gen:\n",
    "                    rxns_attributes.loc[i,'EC Number'] = ec_data_gen[0]\n",
    "                    rxns_attributes.loc[i,'EC Number Link'] = ec_data_gen[1]\n",
    "                    print(f'{rxn} EC Number found: {ec_data_gen[0]}, {ec_data_gen[1]}')\n",
    "            in_bigg+=1\n",
    "        else:\n",
    "            print(rxn,'not in Bigg')\n",
    "            not_bigg+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Reactions from CHOv1 reconstruction {in_chov1}')\n",
    "print(f'Reactions in BiGG database {in_bigg} [not including those in CHOv1)')\n",
    "print(f'Reactions NOT in BiGG {not_bigg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cd563",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#### -------------------------------------------------- ####\n",
    "#### ---- Update Rxns and Attributes Google Sheet ----- ####\n",
    "#### -------------------------------------------------- ####\n",
    "############################################################\n",
    "sheet.update_google_sheet(sheet_rxns, reactions)\n",
    "sheet.update_google_sheet(sheet_attributes, rxns_attributes)\n",
    "print(\"Google Sheet updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8c7aa",
   "metadata": {},
   "source": [
    "## 4. Divide into compartments <a id='compartments'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.io import load_json_model\n",
    "import pandas as pd\n",
    "\n",
    "# model = read_sbml_model('iCHOv3_CHO_05072023.xml')\n",
    "model = load_json_model('iCHOv3_CHO_05072023.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13461d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsystem_list = []\n",
    "for r in model.reactions:\n",
    "    subsystem_list.append(r.subsystem)\n",
    "print(\"Number of unique subsystems:\", len(set(subsystem_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46553ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsystem_counts = pd.Series(subsystem_list).value_counts().reset_index()\n",
    "subsystem_counts.columns = ['Subsystem', 'RXN Counts']\n",
    "subsystem_counts[subsystem_counts['RXN Counts'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsystem_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -Remove blocked reactions from packages\n",
    "#      -Define the high-importance reactions for biomass production\n",
    "#      -Rename subsystems with very few reactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
